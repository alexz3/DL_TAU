{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim # shouldn't be nn?\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [0., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "xor = torch.tensor([[1, 1, 0, 0], [1, 0, 1, 0], [0,1,1,0]], dtype=torch.float)\n",
    "print(xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (fc2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "        self.fc2 = nn.Linear(3, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out \n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate=0.9 #TBD alex, was missing form defineition, 0.9 is a guess.\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "# create a loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [0., 1., 1., 0.]])\n",
      "Test\n",
      "tensor([1., 1.]) tensor([3.6955e-06], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0.]) tensor([1.0000], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1.]) tensor([1.0000], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0.]) tensor([1.3113e-06], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/\n",
    "# run the main training loop\n",
    "\n",
    "print(xor)\n",
    "inputTransposed = xor.transpose(0,1)\n",
    "for r in range(1000):\n",
    "    for row in inputTransposed:\n",
    "        #print(row)    \n",
    "        data = row[:2]\n",
    "        #print(data)\n",
    "        target = row[2]\n",
    "        #print (target)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        #print(net_out)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "print(\"Test\")\n",
    "for row in inputTransposed:\n",
    "    data = row[:2]\n",
    "    target = row[2]\n",
    "    net_out = net(data)\n",
    "    print(data,net_out)\n",
    "#print(net(torch.tensor([xor[0][0],xor[1][0]])))\n",
    "\n",
    "# print(torch.t(xor)[0])\n",
    "# e = Variable(xor)\n",
    "# print (e.view(-1,4))\n",
    "\n",
    "# for x in range(len(xor[0].length):\n",
    "#     print(xor[0][x])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 1*2)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
